{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hrbMn8X1zfW"
   },
   "source": [
    "# Using Recommender Systems to Identify Top Beauty Products\n",
    "\n",
    "Student name: Jonathan Lee\n",
    "\n",
    "Student pace: Full Time\n",
    "\n",
    "Scheduled project review date/time: June 22, 2pm\n",
    "\n",
    "Instructor name: James Irving\n",
    "\n",
    "Blog post URL: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lxhl9NRg1zfW"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This project uses the Surprise package from scikit with Amazon review data of Luxury Beauty products to build a recommendation system. For this analysis, we will examine the performance of memory-based collaborative filtering in the form of K-Nearest Neighbors, as well as of model-based collaborative filtering in the form of Singular Value Decomposition. From our test results, we find that out of KNN methods, Singular Value Decomposition, and Alternating Least Squares methods, Singular Value Decomposition was the best performing model for our selected data. We also examine what the optimal hyperparameters are for this particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKdoCKo41zfX"
   },
   "source": [
    "## Business Problem\n",
    "\n",
    "Our client is a beauty product retailer that wants to know what the most popular products on Amazon are, as well as what other products customers would be likely to give high ratings to, under the assumption that they would give high ratings to these popular products. We want to optimize a recommender system based on Amazon reviews that as accurately as possible predicts other products that customers would be likely to enjoy. Using this optimized recommender system, we will move forward with the goal of using our client's customer preferences to extract insights into what other brands/products would be successful if our client were to add them to their product offering.\n",
    "***\n",
    "Questions to address:\n",
    "* What is are the optimal model and hyperparameters to build a recommender system to work with Amazon ratings dataset to provide recommendations for our own customers?\n",
    "* What are Amazon's most popular products in terms of number of ratings?\n",
    "* Assuming that our client's customers currently give high ratings to the popular products on Amazon, what other products can we recommend adding to inventory?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR7uxb181zfX"
   },
   "source": [
    "## Data Understanding and Preparation\n",
    "\n",
    "In this analysis, we use [Amazon review data](https://nijianmo.github.io/amazon/index.html) and [product metadata](http://deepyeti.ucsd.edu/jianmo/amazon/index.html) featured in the following paper:\n",
    "\n",
    "**Justifying recommendations using distantly-labeled reviews and fined-grained aspects**\n",
    "\n",
    "Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "\n",
    "\n",
    "*Empirical Methods in Natural Language Processing (EMNLP), 2019*\n",
    "\n",
    "Due to the large size of the complete dataset and hardware limitations, we will complete the analysis with only reviews and metadata from the luxury beauty product category.\n",
    "\n",
    "Let's begin by loading in our data and doing some Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:30.123264Z",
     "start_time": "2021-06-26T17:00:28.185893Z"
    },
    "id": "v4HjWyK71zfY"
   },
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:30.133286Z",
     "start_time": "2021-06-26T17:00:30.126830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set theme and style for plots\n",
    "sns.set_theme('talk')\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the Data\n",
    "\n",
    "We have two tables to work with in this analysis:\n",
    "1. Review data: contains product ASIN code, user code, and the rating that user provided.\n",
    "2. Product metadata which includes all product metadata including price, product name, and product images paired with ASIN codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:31.456767Z",
     "start_time": "2021-06-26T17:00:30.144932Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Fe13XBZJ1zfY",
    "outputId": "1b4d0bf0-5652-4cf4-ef16-1978268f3977",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load review dataset and metadata\n",
    "review_df = pd.read_csv('data/Luxury_Beauty.csv', names=['asin', 'user',\n",
    "                                                         'rating', 'timestamp'])\n",
    "meta_df = pd.read_json('data/meta_Luxury_Beauty.json.gz', lines=True)\n",
    "display(review_df, meta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Duplicates and Null Values\n",
    "\n",
    "We are dealing with quite a large dataset, with the number of ratings being over 570,000. Therefore, it will be important to reduce the memory as much as possible by removing unnecessary features and reducing the memory usage. Since the timestamp data is unnecessary to our analysis, we will go ahead and drop that column from our ratings dataset. We also go through an initial iteration of removing duplicates and null values.\n",
    "\n",
    "We will also write a function that displays the size of a dataframe, so that we can confirm that the transformations performed on the dataset are resulting in a reduced memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:31.463529Z",
     "start_time": "2021-06-26T17:00:31.459956Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_df_size(df):\n",
    "    \"\"\"\n",
    "    Gets size of dataframe and prints value in MB.\n",
    "    Function inspired by James Irving.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame) : DataFrame to print size of.\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    size = round((sys.getsizeof(df) * 1e-6), 2)\n",
    "    \n",
    "    print(f\"Dataframe memory usage: {size} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:31.772605Z",
     "start_time": "2021-06-26T17:00:31.466055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates and timestamp column from review table\n",
    "review_df.drop_duplicates(inplace=True)\n",
    "review_df.drop('timestamp', axis=1, inplace=True)\n",
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:31.895468Z",
     "start_time": "2021-06-26T17:00:31.775450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print size of original ratings df\n",
    "get_df_size(review_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly with our metadata, we will go ahead and slice out the ASIN code and product names, since those are the pieces of data that will be used in our analysis. Then, we go on to drop duplicates from this table as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:31.950579Z",
     "start_time": "2021-06-26T17:00:31.897328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Slice asin and title columns from metadata table\n",
    "meta_df = meta_df[['asin','title', 'imageURLHighRes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:31.995465Z",
     "start_time": "2021-06-26T17:00:31.960300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates from metadata table\n",
    "meta_df.drop_duplicates(['asin', 'title'], inplace=True)\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Data Tables\n",
    "\n",
    "Now, we will create a catalog_df which contains all of our ratings combined with their titles. This dataframe contains all of the information we will need for the purpose of our analysis. Let's also keep note of the size of our original catalog_df before we make transformations to reduce the memory allocation, and after dropping any duplicated or null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:32.130344Z",
     "start_time": "2021-06-26T17:00:32.000230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine review data and metadata to create catalog table\n",
    "catalog_df = review_df.merge(meta_df, how='left', on='asin')\n",
    "catalog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:32.534335Z",
     "start_time": "2021-06-26T17:00:32.133241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates from merged catalog table\n",
    "catalog_df.drop_duplicates(['asin', 'user', 'rating', 'title'],inplace=True)\n",
    "catalog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:32.641849Z",
     "start_time": "2021-06-26T17:00:32.536436Z"
    },
    "id": "dlXM_UMA1zfo"
   },
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "catalog_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of null values in this catalog dataframe are quite small, we can go ahead and remove the observations where we do not have a product name paired with its ASIN code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:32.788376Z",
     "start_time": "2021-06-26T17:00:32.644109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "catalog_df.dropna(inplace=True)\n",
    "catalog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:33.024073Z",
     "start_time": "2021-06-26T17:00:32.790571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print size of initial catalog_df\n",
    "get_df_size(catalog_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "\n",
    "In this section, we will proceed to visualize the distribution of our ratings as well as how many users gave how many ratings each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:33.038149Z",
     "start_time": "2021-06-26T17:00:33.026206Z"
    },
    "id": "-9gj06Xk1zfo"
   },
   "outputs": [],
   "source": [
    "# Check distribution of ratings\n",
    "catalog_df['rating'].value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:33.054798Z",
     "start_time": "2021-06-26T17:00:33.041239Z"
    },
    "id": "-9gj06Xk1zfo"
   },
   "outputs": [],
   "source": [
    "# Check distribution of ratings in percent\n",
    "catalog_df['rating'].value_counts(normalize=True).sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:33.633963Z",
     "start_time": "2021-06-26T17:00:33.057856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create bar plot of rating distribution\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "g = sns.histplot(data=catalog_df, x='rating', hue='rating', palette='cool_r',\\\n",
    "                 discrete=True, legend=True)\n",
    "\n",
    "ax.set_title('Distribution of Ratings')\n",
    "ax.set_xlabel('Rating')\n",
    "ax.set_ylabel('Number of Reviews')\n",
    "ax.set_xticks([1,2,3,4,5])\n",
    "ax.legend(['66.3%','12.3%','7.3%','5.1%','8.9%']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:34.836719Z",
     "start_time": "2021-06-26T17:00:33.635746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get number of ratings per user\n",
    "freq_df = catalog_df.groupby('user').agg('count').reset_index()\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:34.912359Z",
     "start_time": "2021-06-26T17:00:34.838879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect measures of central tendency\n",
    "freq_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.028721Z",
     "start_time": "2021-06-26T17:00:34.914193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create table with number of users vs number of ratings per user\n",
    "plot_df = freq_df.groupby('asin').agg('count')[:10]\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.320441Z",
     "start_time": "2021-06-26T17:00:35.032399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create bar plot of users per ratings given\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "g = sns.barplot(data=plot_df, x=plot_df.index, y=plot_df['user'], \\\n",
    "                palette='cool')\n",
    "\n",
    "ax.set_title('Number of Users per Ratings Given')\n",
    "ax.set_xlabel('Ratings Given')\n",
    "ax.set_ylabel('Number of Users')\n",
    "\n",
    "for p in ax.patches:\n",
    "             ax.annotate(\"%.0f\" % p.get_height(), \\\n",
    "                         (p.get_x() + p.get_width() / 2., p.get_height()),\\\n",
    "                          ha='center', va='center', fontsize=13, \\\n",
    "                          color='black', xytext=(0, 5), \\\n",
    "                          textcoords='offset points');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.363118Z",
     "start_time": "2021-06-26T17:00:35.322948Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check measures of central tendency\n",
    "catalog_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31vhPqUh1zfv"
   },
   "source": [
    "### Data Mapping\n",
    "\n",
    "As mentioned before, due to the large size of this dataset, it is important to reduce the data to minimize the amount of memory being used. Hence, we map our ASIN and user codes to integer values in order to optimize memory allocation during the modeling process as well as converting our data types to the smallest possible integer type without losing any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.401667Z",
     "start_time": "2021-06-26T17:00:35.366813Z"
    },
    "id": "MRiILfsS1zfZ"
   },
   "outputs": [],
   "source": [
    "# Create list of unique asin codes\n",
    "asin_list = catalog_df['asin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.415242Z",
     "start_time": "2021-06-26T17:00:35.410216Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4orCzVi1zfa",
    "outputId": "dc70750e-ab2e-4fb8-f364-c774afffcf57"
   },
   "outputs": [],
   "source": [
    "# Create an array of integers to map asin codes to\n",
    "np.arange(len(asin_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.428738Z",
     "start_time": "2021-06-26T17:00:35.423222Z"
    },
    "id": "NSyPjo5L1zfa"
   },
   "outputs": [],
   "source": [
    "# Construct dictionary using asin and corresponding product code\n",
    "asin_map = dict(zip(asin_list, np.arange(len(asin_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.511604Z",
     "start_time": "2021-06-26T17:00:35.431097Z"
    },
    "id": "lop2bm5f1zfb"
   },
   "outputs": [],
   "source": [
    "# Map asin to product code integer and check\n",
    "catalog_df['asin'] = catalog_df['asin'].map(asin_map)\n",
    "catalog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.530411Z",
     "start_time": "2021-06-26T17:00:35.513774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename 'asin' column to 'product_code'\n",
    "catalog_df = catalog_df.rename(columns={'asin': 'product_code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.634237Z",
     "start_time": "2021-06-26T17:00:35.532588Z"
    },
    "id": "X-q2YtoN1zfc"
   },
   "outputs": [],
   "source": [
    "# Create list of unique users\n",
    "user_list = catalog_df['user'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.641263Z",
     "start_time": "2021-06-26T17:00:35.636489Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEL8-GVG1zfc",
    "outputId": "d45b0527-e35c-46af-9320-3c418f093e66"
   },
   "outputs": [],
   "source": [
    "# Create an array of integers to map user codes to\n",
    "np.arange(len(user_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:35.784734Z",
     "start_time": "2021-06-26T17:00:35.643544Z"
    },
    "id": "OAmPLRzM1zfc"
   },
   "outputs": [],
   "source": [
    "# Construct dictionary using user code and corresponding integer\n",
    "user_map = dict(zip(user_list, np.arange(len(user_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:36.996440Z",
     "start_time": "2021-06-26T17:00:35.787325Z"
    },
    "id": "2higw83h1zfd"
   },
   "outputs": [],
   "source": [
    "# Map asin to product code integer and check\n",
    "catalog_df['user'] = catalog_df['user'].map(user_map)\n",
    "catalog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:37.014703Z",
     "start_time": "2021-06-26T17:00:36.999823Z"
    },
    "id": "HQlhMxXo1zfe"
   },
   "outputs": [],
   "source": [
    "# Convert to more efficient integer types\n",
    "catalog_df['rating']=catalog_df['rating'].astype(np.int8)\n",
    "catalog_df['product_code']=catalog_df['product_code'].astype(np.int32)\n",
    "catalog_df['user']=catalog_df['user'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:37.026526Z",
     "start_time": "2021-06-26T17:00:37.017810Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "runCRLkF1zfe",
    "outputId": "83f4b49a-d1d3-41af-8d76-3741dec0b84d"
   },
   "outputs": [],
   "source": [
    "# Check data types\n",
    "catalog_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:37.049653Z",
     "start_time": "2021-06-26T17:00:37.030081Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqTxVtou1zfd",
    "outputId": "2d1fff28-01a3-4159-f5b9-ca69ae437cac"
   },
   "outputs": [],
   "source": [
    "# Check datatype of columns\n",
    "catalog_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have reduced the datasize by converting each feature to its lowest possible integer type, let's take a look at the memory usage of our optimized catalog_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:37.173866Z",
     "start_time": "2021-06-26T17:00:37.052137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print size of transformed and optimized catalog_df\n",
    "get_df_size(catalog_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have successfully reduced the memory usage of this catalog_df from 190.61 MB to 117.31 MB without losing any essential information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Data for Modeling\n",
    "\n",
    "We're almost ready to enter the modeling process, so let's go ahead and slice out just the columns we need to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:37.186980Z",
     "start_time": "2021-06-26T17:00:37.176202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframe with user item rating\n",
    "df = catalog_df[['user', 'product_code', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:37.195229Z",
     "start_time": "2021-06-26T17:00:37.189608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print size of optimized ratings data only\n",
    "get_df_size(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, when we compare our initial ratings df size to our optimized ratings size, we can see that we have gone from 82.72 MB down to 9.11 MB. Much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:37.201766Z",
     "start_time": "2021-06-26T17:00:37.198760Z"
    },
    "id": "Q_OiZ4Sb1zfg"
   },
   "outputs": [],
   "source": [
    "# Save csv file to use in Databricks ALS model\n",
    "# catalog_df.to_csv(r'data/Luxury_Beauty_reduced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnlTse2L1zfw"
   },
   "source": [
    "## Data Modeling\n",
    "\n",
    "In this section, we will take a look at using the Surprise scikit package to test which algorithm will be the best for building a recommender system using our Amazon review data.\n",
    "\n",
    "The models we will look at are some K-Nearest Neighbor models and a series of gridsearched Singular Value Decomposition models. You can find the process behind modeling using Alternating Least Squares in PySpark, but we will leave this model out of our main analysis due to its poor performance on this specific dataset as well as the fact that we will need to use PySpark to perform the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:37.207924Z",
     "start_time": "2021-06-26T17:00:37.204696Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8U_xioXk1zfg",
    "outputId": "2b5e978f-b7de-4d43-db73-88c6af394eb3"
   },
   "outputs": [],
   "source": [
    "# If using Colab, install Surprise\n",
    "# %pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:37.566525Z",
     "start_time": "2021-06-26T17:00:37.211240Z"
    },
    "id": "_CsK7rWl1zfh"
   },
   "outputs": [],
   "source": [
    "# Import necessary packages for building recommender system\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import accuracy\n",
    "from surprise.prediction_algorithms import knns\n",
    "from surprise.similarities import cosine, msd, pearson\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:38.003212Z",
     "start_time": "2021-06-26T17:00:37.568702Z"
    },
    "id": "ZsaA_Lvy1zfh"
   },
   "outputs": [],
   "source": [
    "# Create reader object and format review data for processing\n",
    "reader = Reader(line_format = 'user item rating', sep = ',')\n",
    "data = Dataset.load_from_df(df, reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:39.855330Z",
     "start_time": "2021-06-26T17:00:38.005461Z"
    },
    "id": "TF4p10TJ1zfh"
   },
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T15:01:23.218749Z",
     "start_time": "2021-06-23T15:01:23.211530Z"
    }
   },
   "source": [
    "### Memory-Based Item-Item Collaborative Filtering\n",
    "\n",
    "As we see below, the number of unique items is much less than the number of unique users. Hence, for the following K-Nearest Neighbor models, it will be more effective to use item-based filtering in terms of computational efficiency as well as performance due to the fact that the average rating of each item is less likely to change as quickly as the ratings given by each user to different items. \n",
    "\n",
    "For the KNN Basic and KNN with Means algorithms, we will examine performance based on cosine similarity and Pearson correlation coefficient. However, for the KNN with Z-score and KNN Baseline algorithms, we will only examine the Pearson baseline metric, since the Surprise documentation recommends this in order to achieve the best performance.\n",
    "\n",
    "As we iterate through each model, we will save the resulting mean scores in a cumulative dataframe to be able to easily compare performances and runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:39.865930Z",
     "start_time": "2021-06-26T17:00:39.859458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write function to calculate average test metrics\n",
    "def get_avg_metrics(score_dict):\n",
    "    \"\"\"\n",
    "    Calculates average of each list in the specified dictionary.\n",
    "    \n",
    "    Inspired by solution by Jiby on StackOverflow:\n",
    "    https://stackoverflow.com/questions/30687244/python-3-4-how-to-get-the-average-of-dictionary-values\n",
    "\n",
    "    Args:\n",
    "        score_dict (dict) : Dictionary with model test scores.\n",
    "        \n",
    "    Returns:\n",
    "        avgDict (dict) : Dictionary with calculated mean average values.\n",
    "    \"\"\"\n",
    "    \n",
    "    avgDict = {}\n",
    "    for k,v in score_dict.items():\n",
    "        avgDict[k] = sum(v)/ float(len(v))\n",
    "    return avgDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:39.873613Z",
     "start_time": "2021-06-26T17:00:39.869685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize cumulative results dataframe\n",
    "cumulative_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:39.894302Z",
     "start_time": "2021-06-26T17:00:39.877865Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zgI5uO81zfd",
    "outputId": "f715fd6c-9354-4e55-c797-88d4effc5578"
   },
   "outputs": [],
   "source": [
    "# Check how many unique values for asin\n",
    "catalog_df['product_code'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:39.919323Z",
     "start_time": "2021-06-26T17:00:39.896942Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQbokPo51zfe",
    "outputId": "7822e3e7-c2a0-407b-f446-cdeb4b4870cc"
   },
   "outputs": [],
   "source": [
    "# Check how many unique values for user\n",
    "catalog_df['user'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsmhNikn1zfh"
   },
   "source": [
    "#### KNN Basic\n",
    "\n",
    "We begin with the most basic form of the K-Nearest Neighbors algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:45.110733Z",
     "start_time": "2021-06-26T17:00:39.921452Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBSij9YT1zfi",
    "outputId": "5072d6a2-ff3e-486a-ba53-570a77af043c"
   },
   "outputs": [],
   "source": [
    "# KNN Basic with cosine similarity\n",
    "KNN_basic_cos = knns.KNNBasic(sim_options={'name': 'cosine', \n",
    "                                          'user_based': False}).fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:00:46.995945Z",
     "start_time": "2021-06-26T17:00:45.113034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions on test data and print RMSE and MAE\n",
    "predictions = KNN_basic_cos.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:01:31.980032Z",
     "start_time": "2021-06-26T17:00:46.998048Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with cross validated average scores\n",
    "KNN_basic_cos_dict = cross_validate(KNN_basic_cos, data, verbose= True, \\\n",
    "                                    n_jobs=-1)\n",
    "KNN_basic_cos_dict = get_avg_metrics(KNN_basic_cos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:01:32.136386Z",
     "start_time": "2021-06-26T17:01:31.989707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(KNN_basic_cos_dict, index=[\"KNN_basic_cos\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a starting point, let's compare how using the Pearson correlation coefficient as our similarity measure alters the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:01:38.043796Z",
     "start_time": "2021-06-26T17:01:32.138654Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcQiQwWC1zfi",
    "outputId": "7c7f270a-54bd-4be9-e427-33bb91e2727b"
   },
   "outputs": [],
   "source": [
    "# KNN Basic with pearson correlation similarity\n",
    "KNN_basic_pearson = knns.KNNBasic(sim_options={'name': 'pearson', \n",
    "                                              'user_based': False})\\\n",
    "                        .fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:01:39.398111Z",
     "start_time": "2021-06-26T17:01:38.048150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions on test data and print RMSE and MAE\n",
    "predictions = KNN_basic_pearson.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:02:24.575524Z",
     "start_time": "2021-06-26T17:01:39.400388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with cross validated average scores\n",
    "KNN_basic_pearson_dict = cross_validate(KNN_basic_pearson, \\\n",
    "                                        data, verbose= True, n_jobs=-1)\n",
    "KNN_basic_pearson_dict = get_avg_metrics(KNN_basic_pearson_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:02:24.646613Z",
     "start_time": "2021-06-26T17:02:24.582402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(KNN_basic_pearson_dict, index=[\"KNN_basic_pearson\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a slightly lower RMSE when we use the Pearson correlation coefficient on the KNN basic algorithm. Although the fit time is quite a bit longer than when we used the cosine similarity, this difference is not large enough for us to sacrifice a lower RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfi9P0Gu1zfj"
   },
   "source": [
    "#### KNN With Means\n",
    "\n",
    "Next, we move onto a KNN algorithm which takes into account the mean ratings of each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:02:30.321972Z",
     "start_time": "2021-06-26T17:02:24.650381Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2elEhOw1zfj",
    "outputId": "5584db98-6271-4338-c5b1-9e040a036c06"
   },
   "outputs": [],
   "source": [
    "# KNN with Means with cosine similarity\n",
    "KNN_mean_cos = knns.KNNWithMeans(sim_options={'name': 'cosine', \\\n",
    "                                              'user_based': False})\\\n",
    "                   .fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:02:31.723231Z",
     "start_time": "2021-06-26T17:02:30.324128Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions on test data and print RMSE and MAE\n",
    "predictions = KNN_mean_cos.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:03:09.648715Z",
     "start_time": "2021-06-26T17:02:31.726349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with cross validated average scores\n",
    "KNN_mean_cos_dict = cross_validate(KNN_mean_cos, data, verbose= True, \\\n",
    "                                   n_jobs=-1)\n",
    "KNN_mean_cos_dict = get_avg_metrics(KNN_mean_cos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:03:09.699388Z",
     "start_time": "2021-06-26T17:03:09.654584Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(KNN_mean_cos_dict, index=[\"KNN_mean_cos\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that our KNN with means using cosine similarity is not able to achieve a better score than our KNN basic with Pearson's correlation coefficient. Let's see what happens when we use the Pearson correlation coefficient on KNN with means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:03:15.703769Z",
     "start_time": "2021-06-26T17:03:09.702838Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3OuWN8W1zfk",
    "outputId": "5dadc38c-842b-448b-cfb8-b0d22426e126"
   },
   "outputs": [],
   "source": [
    "# KNN with Means with pearson correlation similarity\n",
    "KNN_mean_pearson = knns.KNNWithMeans(sim_options={'name': 'pearson', \\\n",
    "                                                  'user_based': False})\\\n",
    "                       .fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:03:17.557893Z",
     "start_time": "2021-06-26T17:03:15.707637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions on test data and print RMSE and MAE\n",
    "predictions = KNN_mean_pearson.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:04:00.896782Z",
     "start_time": "2021-06-26T17:03:17.560581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with cross validated average scores\n",
    "KNN_mean_pearson_dict = cross_validate(KNN_mean_pearson, data, verbose= True,\\\n",
    "                                       n_jobs=-1)\n",
    "KNN_mean_pearson_dict = get_avg_metrics(KNN_mean_pearson_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:04:00.938176Z",
     "start_time": "2021-06-26T17:04:00.901040Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(KNN_mean_pearson_dict, index=[\"KNN_mean_pearson\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we still do not have a better RMSE than our KNN basic with Pearson's correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfi9P0Gu1zfj"
   },
   "source": [
    "#### KNN With Z-Score\n",
    "\n",
    "This algorithm takes into account the Z-score normalization of each item's ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:04:09.242193Z",
     "start_time": "2021-06-26T17:04:00.940731Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3OuWN8W1zfk",
    "outputId": "5dadc38c-842b-448b-cfb8-b0d22426e126"
   },
   "outputs": [],
   "source": [
    "# KNN with Z-score with pearson baseline correlation similarity\n",
    "KNN_z_pearson = knns.KNNWithZScore(sim_options={'name': 'pearson_baseline', \\\n",
    "                                                'user_based': False})\\\n",
    "                    .fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:04:11.042231Z",
     "start_time": "2021-06-26T17:04:09.245091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions on test data and print RMSE and MAE\n",
    "predictions = KNN_z_pearson.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:04:50.645020Z",
     "start_time": "2021-06-26T17:04:11.044197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with cross validated average scores\n",
    "KNN_z_pearson_dict = cross_validate(KNN_z_pearson, data, verbose= True, \\\n",
    "                                    n_jobs=-1)\n",
    "KNN_z_pearson_dict = get_avg_metrics(KNN_z_pearson_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:04:50.693659Z",
     "start_time": "2021-06-26T17:04:50.650643Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(KNN_z_pearson_dict, index=[\"KNN_z_pearson\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN with Z-score using the Pearson's correlation coefficient seems to be yielding a slightly better RMSE than most models, and has a very similar score and fit time to our KNN basic with Pearson's correlation coefficient. However, KNN basic with Pearson's correlation coefficient is still our best algorithm to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfi9P0Gu1zfj"
   },
   "source": [
    "#### KNN Baseline\n",
    "\n",
    "This final algorithm is a K-Nearest Neighbors algorithm that takes into account a baseline rating for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:04:59.314418Z",
     "start_time": "2021-06-26T17:04:50.696859Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3OuWN8W1zfk",
    "outputId": "5dadc38c-842b-448b-cfb8-b0d22426e126"
   },
   "outputs": [],
   "source": [
    "# KNN Baseline with pearson baseline similarity\n",
    "KNN_base_pearson= knns.KNNBaseline(sim_options={'name': 'pearson_baseline', \\\n",
    "                                                'user_based': False})\\\n",
    "                      .fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:05:00.997306Z",
     "start_time": "2021-06-26T17:04:59.318211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions on test data and print RMSE and MAE\n",
    "predictions = KNN_base_pearson.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:05:36.632360Z",
     "start_time": "2021-06-26T17:05:01.022111Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with cross validated average scores\n",
    "KNN_base_pearson_dict = cross_validate(KNN_base_pearson, data, \\\n",
    "                                       verbose= True, n_jobs=-1)\n",
    "KNN_base_pearson_dict = get_avg_metrics(KNN_base_pearson_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:05:36.683311Z",
     "start_time": "2021-06-26T17:05:36.640789Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(KNN_base_pearson_dict, index=[\"KNN_base_pearson\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to our last KNN baseline algorithm, all other KNN algorithms seem to have a similar RMSE score across the board. Hence, we have a clear winner with our KNN baseline using Pearson's correlation coefficient having the best RMSE and MAE out of all other KNN algorithms that we have examined to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Based Collaborative Filtering via Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9OnptzQ1zfk"
   },
   "source": [
    "#### Singular Value Decomposition\n",
    "\n",
    "Now, let's move onto the SVD model where we will begin with a basic model and try to improve our score by using a series of gridsearches. This model-based approach takes a sparse matrix where we have users x items, and decomposes this utility matrix into item characteristics and user preferences that correspond to those characteristics. By utilizing a gridsearch, we can determine the optimal number of factors, or characteristics/preferences, as well as adjust learning and regularization rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:06:01.328609Z",
     "start_time": "2021-06-26T17:05:36.686424Z"
    },
    "id": "fp88ZZzN1zfk"
   },
   "outputs": [],
   "source": [
    "# Train basic SVD model\n",
    "svd1 = SVD(random_state=27)\n",
    "svd1.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:06:03.088800Z",
     "start_time": "2021-06-26T17:06:01.331075Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrTqOsvG1zfl",
    "outputId": "948c0691-c84a-40f8-9bd0-52d74843e23e"
   },
   "outputs": [],
   "source": [
    "# Get predictions on test data and print RMSE\n",
    "predictions = svd1.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:07:03.805195Z",
     "start_time": "2021-06-26T17:06:03.093725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with average scores\n",
    "svd1_dict = cross_validate(svd1, data, verbose= True, n_jobs=-1)\n",
    "svd1_dict = get_avg_metrics(svd1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:07:03.829041Z",
     "start_time": "2021-06-26T17:07:03.808235Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(svd1_dict, index=[\"svd1\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad start for a basic SVD model. We have a slightly higher RMSE than our best KNN model. However, we should also note that our fit time is quite a bit longer than any of our memory-based models. Let's go about trying to optimize our SVD model for a better RMSE by using a series of grid searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:24:01.023464Z",
     "start_time": "2021-06-26T17:07:03.831177Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Ho9k88v1zfl",
    "outputId": "ae2fe196-b5f1-4b7e-f6b7-611608cd8afe"
   },
   "outputs": [],
   "source": [
    "# Gridsearch #1\n",
    "param_grid = {'n_factors':[110, 130],'n_epochs': [25, 30], \\\n",
    "              'lr_all': [0.025, 0.05], 'reg_all': [0.1, 0.2]}\n",
    "svd_grid1 = GridSearchCV(SVD,param_grid=param_grid,joblib_verbose=5, \\\n",
    "                         n_jobs=-1)\n",
    "svd_grid1.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:24:01.038152Z",
     "start_time": "2021-06-26T17:24:01.028562Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zl9extk1zfl",
    "outputId": "cadb82cd-7d65-4a04-c03e-ec829e2a6e35"
   },
   "outputs": [],
   "source": [
    "# Print results from gridsearch #1\n",
    "svd_grid1.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:24:43.511084Z",
     "start_time": "2021-06-26T17:24:01.041535Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAgz8xbd1zfl",
    "outputId": "7bf64c06-d55c-4418-8b1b-d58d4a2ede16"
   },
   "outputs": [],
   "source": [
    "# Use best params to get RMSE and MAE on test data\n",
    "svd2 = SVD(n_factors=130, n_epochs=30, lr_all=0.025, reg_all=0.1, \\\n",
    "           random_state=27)\n",
    "svd2.fit(trainset)\n",
    "predictions = svd2.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:26:04.509009Z",
     "start_time": "2021-06-26T17:24:43.519887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with average scores\n",
    "svd2_dict = cross_validate(svd2, data, verbose= True, n_jobs=-1)\n",
    "svd2_dict = get_avg_metrics(svd2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:26:04.536848Z",
     "start_time": "2021-06-26T17:26:04.511398Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(svd2_dict, index=[\"svd2\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we see that our fit times are becoming relatively long, after just one grid search, we already have our best RMSE out of both memory-based and model-based algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:48:54.941071Z",
     "start_time": "2021-06-26T17:26:04.540316Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ck07DV71zfm",
    "outputId": "8a4b1be8-bffc-49c3-aa00-3be0f78ff52f"
   },
   "outputs": [],
   "source": [
    "# Gridsearch #2\n",
    "param_grid = {'n_factors':[130, 150],'n_epochs': [30, 40], \\\n",
    "              'lr_all': [0.01, 0.025], 'reg_all': [0.05, 0.1]}\n",
    "svd_grid2 = GridSearchCV(SVD,param_grid=param_grid,joblib_verbose=5, \\\n",
    "                         n_jobs=-1)\n",
    "svd_grid2.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:48:54.957628Z",
     "start_time": "2021-06-26T17:48:54.947608Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUwl75Li-rGw",
    "outputId": "3502e2b2-620d-4b1a-e3f1-72dba2572219"
   },
   "outputs": [],
   "source": [
    "# Print results from gridsearch #2\n",
    "svd_grid2.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:49:56.196446Z",
     "start_time": "2021-06-26T17:48:54.961103Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h91zkVXT-rka",
    "outputId": "923cdd40-74cc-40e9-bdcd-097a67b54ab9"
   },
   "outputs": [],
   "source": [
    "# Use best params to get RMSE and MAE on test data\n",
    "svd3 = SVD(n_factors=150, n_epochs=40, lr_all=0.025, reg_all=0.1, \\\n",
    "           random_state=27)\n",
    "svd3.fit(trainset)\n",
    "predictions = svd3.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:51:48.261452Z",
     "start_time": "2021-06-26T17:49:56.205349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with average scores\n",
    "svd3_dict = cross_validate(svd3, data, verbose= True, n_jobs=-1)\n",
    "svd3_dict = get_avg_metrics(svd3_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T17:51:48.285660Z",
     "start_time": "2021-06-26T17:51:48.264661Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(svd3_dict, index=[\"svd3\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RMSE and MAE scores continue to get better with each grid search, but it looks like our RMSE is only improving marginally in comparison to the amount of additional time it is taking to fit our models. We will proceed to do one final grid search to see if we can improve our RMSE by just a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:00:30.325416Z",
     "start_time": "2021-06-26T17:51:48.288242Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1NzBPmg1zfm",
    "outputId": "78375387-3115-46f7-91f5-c18d98f29033"
   },
   "outputs": [],
   "source": [
    "# Gridsearch #3\n",
    "param_grid = {'n_factors':[150, 200],'n_epochs': [40, 50], 'lr_all': [0.025],\n",
    "              'reg_all': [0.1]}\n",
    "svd_grid_final = GridSearchCV(SVD,param_grid=param_grid,joblib_verbose=5, \\\n",
    "                              n_jobs=-1)\n",
    "svd_grid_final.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:00:30.335334Z",
     "start_time": "2021-06-26T18:00:30.329447Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVmQ9UCi1zfm",
    "outputId": "dae08dd0-ab1e-471e-df8d-a509287f244b"
   },
   "outputs": [],
   "source": [
    "# Print results from final gridsearch\n",
    "svd_grid_final.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:01:47.504389Z",
     "start_time": "2021-06-26T18:00:30.338702Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6u_LywKw1zfm",
    "outputId": "4ff2e60f-10bd-4862-c64c-c3e7f0418678"
   },
   "outputs": [],
   "source": [
    "# Use best params to get RMSE and MAE on test data\n",
    "svd_final = SVD(lr_all=0.025, n_epochs=50, n_factors=150, reg_all=0.1, \\\n",
    "                random_state=27)\n",
    "svd_final.fit(trainset)\n",
    "predictions = svd_final.test(testset)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:04:00.207120Z",
     "start_time": "2021-06-26T18:01:47.508467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save dictionary with average scores\n",
    "svd_final_dict = cross_validate(svd_final, data, verbose= True, n_jobs=-1)\n",
    "svd_final_dict = get_avg_metrics(svd_final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:04:00.238363Z",
     "start_time": "2021-06-26T18:04:00.215637Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df from row of mean results to append to cumulative df\n",
    "row_to_df = pd.DataFrame(svd_final_dict, index=[\"svd_final\"])\n",
    "cumulative_results = cumulative_results.append(row_to_df)\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final SVD model has the best RMSE and MAE to this point. Although it has a significantly longer fit time than some of the KNN models, we also see that the time it takes to get predictions is the shortest. Because we can fit our data to our final SVD prior to getting predictions in a practical use case, longer fit time will not be a problem. Hence, we will move forward with our SVD model with the best RMSE and MAE scores and the following hyperparameters:\n",
    "1. lr_all=0.025\n",
    "2. n_epochs=50\n",
    "3. n_factors=150\n",
    "4. reg_all=0.1\n",
    "\n",
    "Let's also fit our whole dataset to the model and pickle it to easily get predictions from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:05:40.232534Z",
     "start_time": "2021-06-26T18:04:00.241079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train SVD model using best hyperparameters on full dataset\n",
    "svd_final = SVD(lr_all=0.025, n_epochs=50, n_factors=150, reg_all=0.1,\n",
    "                random_state=27)\n",
    "svd_final.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:05:40.237380Z",
     "start_time": "2021-06-26T18:05:40.234869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickle svd_final\n",
    "# import pickle\n",
    "\n",
    "# with open('svdfinal.pickle', 'wb') as f:\n",
    "#     pickle.dump(svd_final, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CysMGW861zfw"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "In this section, we will begin by evaluating our test scores and then move on to build some functions to assist the client in looking up product codes. Finally, we will build a recommender system that takes a list of preferred products and returns a list of items that the user would likely give a high rating to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our test scores from all of the models that we've fit to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:05:40.258633Z",
     "start_time": "2021-06-26T18:05:40.239639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display all mean scores\n",
    "cumulative_results.style.background_gradient(cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can see that by using our gridsearches, we were able to make some improvements in the RMSE score between iterations. We also see that our final SVD model has a lower RMSE score than even our best performing KNN Baseline model, so we will move forward to building our recommender system using the SVD model with the best parameters found in our final gridsearch. We can also see that our MAE score is 0.9230, meaning that in terms of rating stars, the average error of our model is off by 0.9230 stars from the actual rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zmu1xxrE1zfr"
   },
   "source": [
    "### Searching Product Codes\n",
    "\n",
    "Here, we create a reduced catalog of product names with their corresponding product codes. We then build a function to search the name of a product to assist our user in looking up product codes to input into the recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:05:40.286895Z",
     "start_time": "2021-06-26T18:05:40.260486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set pandas options to increase max column width and row number\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.max_rows = 500\n",
    "catalog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:05:40.315229Z",
     "start_time": "2021-06-26T18:05:40.289173Z"
    }
   },
   "outputs": [],
   "source": [
    "catalog_df['imageURLHighRes'][5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:05:40.436543Z",
     "start_time": "2021-06-26T18:05:40.317323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create lookup df to look up product codes and/or names\n",
    "lookup_df = catalog_df.drop_duplicates('product_code')\n",
    "lookup_df = lookup_df[['product_code', 'title', 'imageURLHighRes']]\n",
    "lookup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:05:40.442692Z",
     "start_time": "2021-06-26T18:05:40.438388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create function to look up product codes\n",
    "def product_search():\n",
    "    \"\"\"\n",
    "    Prompts user to look up product name and returns product code.\n",
    "\n",
    "    Args:\n",
    "        \n",
    "    Returns:\n",
    "        search_results (DataFrame) : DataFrame including results of searched \n",
    "        product name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prompt user for item name\n",
    "    query_product = input('Search a brand or product: ')\n",
    "    \n",
    "    # Prompt user for number of results desired\n",
    "    num_results = int(input('Up to how many results would you like to see? '))\n",
    "    \n",
    "    # Slice catalog_df to return DataFrame with results containing query\n",
    "    search_results = lookup_df[lookup_df['title'].str\\\n",
    "                            .contains(query_product, case=False, na=False)]\\\n",
    "                            .head(num_results)\n",
    "    \n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:20:04.921354Z",
     "start_time": "2021-06-27T00:19:55.976389Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look up sample product codes\n",
    "product_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Recommender System\n",
    "\n",
    "In this section, we will take the hyperparameters from our best performing SVD model to build a usable recommender system. Upon running the function, the user will be prompted to enter a list of product codes of products that they gave high ratings to, and they will be given a list of products that our algorithm would recommend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in our pickled final model and begin by creating a function that displays Amazon's existing customers' ratings as well as our recommendations for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.075714Z",
     "start_time": "2021-06-26T17:00:28.585Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load in pickled final model\n",
    "# with open('svdfinal.pickle', 'rb') as file:\n",
    "#     model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.077397Z",
     "start_time": "2021-06-26T17:00:28.589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create function to train model on full dataset and return recommendations\n",
    "def existing_user_ratings(model, user_no, num_res=5):\n",
    "    \"\"\"\n",
    "    Prompts user to enter customer's preferred product codes, models SVD\n",
    "    using ideal hyperparameters, and returns however many predictions\n",
    "    the user requests.\n",
    "\n",
    "    Args:\n",
    "        model : Pre-trained model to pull predictions from.\n",
    "        user_no (int) : Specific user to provide recommendations for.\n",
    "        num_res (int) : Number of recommendations to display. Default value is\n",
    "        5 recommendations.\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Create total list of predictions for new user\n",
    "    list_of_predictions = []\n",
    "    for item in df['product_code'].unique():\n",
    "        list_of_predictions.append((item, model.predict(user_no, item)[3]))\n",
    "    \n",
    "    # Sort predictions from high to low\n",
    "    ranked_predictions = sorted(list_of_predictions, key=lambda x:x[1], \\\n",
    "                                reverse=True)\n",
    "    \n",
    "    # Create dataframe from ranked predictions\n",
    "    ranked_df = pd.DataFrame(ranked_predictions, columns=['product_code', \\\n",
    "                                                          'rating'])\n",
    "    \n",
    "    # Merge predictions with lookup df to get product names\n",
    "    merged_df = ranked_df.merge(lookup_df, how='inner', on='product_code')\n",
    "    \n",
    "    # Create dataframe with requested number of results\n",
    "    rec_list = merged_df.head(num_res)\n",
    "    \n",
    "    # Get user's ratings\n",
    "    user_rated = catalog_df[catalog_df['user']==user_no]\n",
    "    display('Customer has rated the following products: ', user_rated)\n",
    "    \n",
    "    # Get list of user's products\n",
    "    prod_list = user_rated['product_code'].tolist()\n",
    "    \n",
    "    # Remove products that user has already rated\n",
    "    for prod in prod_list:\n",
    "        rec_list = rec_list[rec_list['product_code'] != prod]\n",
    "    \n",
    "    display('Recommendations for customer: ', rec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.079062Z",
     "start_time": "2021-06-26T17:00:28.592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get recommendations for user 27000\n",
    "existing_user_ratings(model, 27000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.080871Z",
     "start_time": "2021-06-26T17:00:28.596Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get recommendations for user 42424\n",
    "existing_user_ratings(model, 42424)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a recommender system function for new users to be able to input their own product ratings, and get new recommended products from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.082652Z",
     "start_time": "2021-06-26T17:00:28.600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check last user number\n",
    "df['user'].sort_values().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:23:04.816786Z",
     "start_time": "2021-06-27T00:23:04.804207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create function to train model on full dataset and return recommendations\n",
    "def user_ratings(lr_all=0.025, n_epochs=50, n_factors=150, reg_all=0.1,\n",
    "                 random_state=27):\n",
    "    \"\"\"\n",
    "    Prompts user to enter customer's preferred product codes, models SVD\n",
    "    using ideal hyperparameters, and returns however many predictions\n",
    "    the user requests.\n",
    "\n",
    "    Args:\n",
    "        lr_all : The learning rate for all parameters. Default is ``0.025``.\n",
    "        n_epochs : The number of iteration of the SGD procedure. Default is \n",
    "            ``50``.\n",
    "        n_factors : The number of factors. Default is ``150``.\n",
    "        reg_all : The regularization term for all parameters. Default is \n",
    "            ``0.1``.\n",
    "        random_state (int) : Determines the RNG that will be used for \n",
    "            initialization. If int, ``random_state`` will be used as a seed \n",
    "            for a new RNG. This is useful to get the same initialization over \n",
    "            multiple calls to ``fit()``.  If RandomState instance, this same \n",
    "            instance is used as RNG. If ``None``, the current RNG from numpy \n",
    "            is used.  Default is``27``.\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Prompt user for number of products they want to review\n",
    "    num_ratings = int(input(\"How many products would you like to rate? \"))\n",
    "    product_ratings = []\n",
    "    \n",
    "    # Prompt user for product code and its rating\n",
    "    for rating in range(0, num_ratings):\n",
    "        ind_prod_rating = [int(x) for x in \\\n",
    "                       input('Enter product code followed by its rating out of 5 (separate by spaces): ')\\\n",
    "                       .split()]\n",
    "        product_ratings.append({ind_prod_rating[0]:ind_prod_rating[1]})\n",
    "    \n",
    "    # Prompt user for desired number of product recommendations\n",
    "    num_res = int(input('How many recommendations would you like to see? '))\n",
    "    \n",
    "    # Create list of ratings to add to dataset\n",
    "    keys = []\n",
    "    for d in product_ratings:\n",
    "        keys.extend(d.keys())\n",
    "        \n",
    "    values = []\n",
    "    for d in product_ratings:\n",
    "        values.extend(d.values())\n",
    "    \n",
    "    user_rating_list = []\n",
    "    for rating in range(0, num_ratings):\n",
    "        user_rating_list.append({'user': 600000, 'product_code': keys[rating],\n",
    "                                 'rating': values[rating]})\n",
    "    \n",
    "    # Add new ratings to full dataset\n",
    "    new_ratings_df = df.append(user_rating_list, ignore_index=True)\n",
    "    \n",
    "    # Format dataset for modeling\n",
    "    reader = Reader(line_format='user item rating')\n",
    "    new_data = Dataset.load_from_df(new_ratings_df, reader)\n",
    "    \n",
    "    # Train model on full dataset using preset hyperparameters\n",
    "    svd_ = SVD(lr_all=lr_all, n_epochs=n_epochs, n_factors=n_factors, \\\n",
    "               reg_all=reg_all, random_state=random_state)\n",
    "    svd_.fit(new_data.build_full_trainset())\n",
    "    \n",
    "    # Create total list of predictions for new user\n",
    "    list_of_predictions = []\n",
    "    for item in df['product_code'].unique():\n",
    "        list_of_predictions.append((item, svd_.predict(600000, item)[3]))\n",
    "    \n",
    "    # Sort predictions from high to low\n",
    "    ranked_predictions = sorted(list_of_predictions, key=lambda x:x[1], \\\n",
    "                                reverse=True)\n",
    "    \n",
    "    # Create dataframe from ranked predictions\n",
    "    ranked_df = pd.DataFrame(ranked_predictions, columns=['product_code', \\\n",
    "                                                          'rating'])\n",
    "    \n",
    "    # Merge predictions with lookup df to get product names\n",
    "    merged_df = ranked_df.merge(lookup_df, how='inner', on='product_code')\n",
    "    \n",
    "    # Create dataframe with requested number of results\n",
    "    rec_list = merged_df.head(num_res)\n",
    "    \n",
    "    # Get user's ratings and display\n",
    "    user_rated = new_ratings_df[new_ratings_df['user']==600000]\n",
    "    user_rated_lookup = user_rated.merge(lookup_df, how='inner', on='product_code')\n",
    "    display('Customer has rated the following products: ', user_rated_lookup)\n",
    "    \n",
    "    # Get list of user's products\n",
    "    prod_list = user_rated['product_code'].tolist()\n",
    "    \n",
    "    # Remove products that user has already rated\n",
    "    for prod in prod_list:\n",
    "        rec_list = rec_list[rec_list['product_code'] != prod]\n",
    "        \n",
    "    # Display recommendations\n",
    "    display('Recommendations for customer: ', rec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:25:07.573482Z",
     "start_time": "2021-06-27T00:23:05.726582Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "user_ratings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have our product recommendations! Now, let's take a look at what the top products were by selecting the top 10 products in number of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.087265Z",
     "start_time": "2021-06-26T17:00:28.612Z"
    }
   },
   "outputs": [],
   "source": [
    "# View top 10 products with most reviews\n",
    "top_series = catalog_df['product_code'].value_counts().head(10)\n",
    "top_df = pd.DataFrame(top_series)\n",
    "top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.088869Z",
     "start_time": "2021-06-26T17:00:28.616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create list of top 10 products with most reviews\n",
    "top_list = catalog_df['product_code'].value_counts().index[:10].tolist()\n",
    "top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.090192Z",
     "start_time": "2021-06-26T17:00:28.622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge top_df with lookup_df\n",
    "new_df = top_df.merge(lookup_df, how='left', left_index=True, \\\n",
    "                      right_on='product_code')\n",
    "new_df = new_df.groupby('title').agg({'product_code_x':'sum'})\\\n",
    "                                .sort_values(by='product_code_x', \\\n",
    "                                             ascending=False)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.091318Z",
     "start_time": "2021-06-26T17:00:28.626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Limit title length to 45 characters\n",
    "new_df.index = new_df.index.str[:45]\n",
    "new_df = new_df.reset_index()\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.092528Z",
     "start_time": "2021-06-26T17:00:28.630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create bar plot most popular products\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "g = sns.barplot(data=new_df, x='title', y='product_code_x', palette='cool', \\\n",
    "                ci=None)\n",
    "\n",
    "ax.set_title('Number of Users per Ratings Given')\n",
    "ax.set_xlabel('Ratings Given')\n",
    "ax.set_ylabel('Number of Users')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "for p in ax.patches:\n",
    "             ax.annotate(\"%.0f\" % p.get_height(), \\\n",
    "                         (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                 ha='center', va='center', fontsize=13, color='black', \\\n",
    "                         xytext=(0, 5),\n",
    "                 textcoords='offset points');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that our client already carries these products which are popular on Amazon, let's see what other product recommendations we can get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T00:12:16.093556Z",
     "start_time": "2021-06-26T17:00:28.634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get final recommendations\n",
    "user_ratings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-ZVAUzf1zfw"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "And there we have our final product recommendations! We can see that the Singular Value Decomposition had the best performance with respect to RMSE. Upon running a series of gridsearches, we were also able to determine the optimal hyperparameters to further reduce the RMSE score. \n",
    "\n",
    "To interpret our error, we looked at the MAE score which was 0.9237 on our final best model, meaning that  the average error of our model is off by 0.9237 stars from the actual rating.\n",
    "\n",
    "Finally, we built out functions to help us look up product codes to put into a recommender system which would then provide us with however many product recommendations the user desires.\n",
    "\n",
    "The value of this project lies in the ability to use Amazon's huge amount of ratings data to identify what other products a smaller retailer might want to consider adding to their inventory. The only additional data that we would need from the retailer would be customer preferences on the products that the retailer currently carries and that the customer would give high ratings to, and we can place this information in the context of Amazon's ratings to determine what other products this customer would be likely to give high ratings to.\n",
    "\n",
    "A limitation to this analysis is that the dataset only contains beauty products under the \"Luxury Beauty\" category, which is a collection of approved brands. Amazon also has a category labeled \"All Beauty\" whose data we have omitted in this analysis due to hardware limitations that would occur under the stress of dealing with the such a large size of these combined datasets.\n",
    "\n",
    "To summarize, here are the final recommendations for our client:\n",
    "\n",
    "1. In order to build a similar recommender system, SVD would be the best algorithm to use, with the following hyperparameters: lr_all=0.025, n_epochs=50, n_factors=150, reg_all=0.1\n",
    "2. Client should carry the following products based on popularity on Amazon:\n",
    "* TOPPIK Hair Building Fibers\n",
    "* HOT TOOLS Professional 24k Gold Extra-Long Barrel Curling Iron/Wand\n",
    "* Mario Badescu Facial Spray with Aloe, Herbs and Rosewater\n",
    "* OPI Nail Lacquer, Cajun Shrimp\n",
    "* OPI Nail Lacquer, Not So Bora-Bora-ing Pink\n",
    "* BaBylissPRO Ceramix Xtreme Dryer\n",
    "* OPI Nail Envy Nail Strengthener\n",
    "* Proraso Shaving Soap in a Bowl, Refreshing and Toning\n",
    "\n",
    "3. Assuming that our client's current customers would give high ratings to those products, our client should also consider carrying the following products:\n",
    "* Crabtree & Evelyn - Gardener's Ultra-Moisturising Hand Therapy Pump\n",
    "* Crabtree & Evelyn Hand Soap, Gardeners\n",
    "* Soy Milk Hand Crme\n",
    "* Paul Mitchell Shampoo One\n",
    "* Glytone Rejuvenating Mask\n",
    "* PCA SKIN Protecting Hydrator Broad Spectrum SPF 30\n",
    "* jane iredale Amazing Base Loose Mineral Powder\n",
    "* jane iredale So-Bronze, Bronzing Powder\n",
    "* YU-Be: Japan’s secret for dry skin relief. Deep hydrating moisturizing cream for face, hand and body\n",
    "* Calvin Klein ETERNITY Eau de Parfum\n",
    "\n",
    "\n",
    "\n",
    "Although ALS has been proven to be an effective algorithm in recommender systems, it was surprising to see such a poor performance score with the data used in this analysis. Moving forward, it might be a worthwhile investigation to see how the model performs if we combine data from the \"All Beauty\" category with the data used in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "name": "dsc-phase4-project-rougheda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "373.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
